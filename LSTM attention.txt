Вот пример кода для прогнозирования временных рядов с использованием нейронной сети с механизмом Attention на основе модели LSTM (Long Short-Term Memory) в TensorFlow/Keras. Механизм Attention может помочь нейронной сети сосредоточиться на наиболее важных временных шагах, что может улучшить качество прогнозов.

Для этого мы используем LSTM для обработки временного ряда и добавляем механизм Attention.

Установка необходимых библиотек:
bash
Копировать код
pip install tensorflow numpy matplotlib scikit-learn
Код:
python
Копировать код
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Activation, dot, concatenate
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Генерация или загрузка данных
# Предположим, что данные о температуре хранятся в массиве temperatures
# Пример создания случайных данных:
np.random.seed(42)
temperatures = np.random.uniform(low=-10, high=30, size=(72,))

# Масштабирование данных
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_temperatures = scaler.fit_transform(temperatures.reshape(-1, 1))

# Создание временных окон
def create_dataset(data, time_steps=5):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:(i + time_steps), 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)

time_steps = 5
X, y = create_dataset(scaled_temperatures, time_steps)

# Добавляем размерность для LSTM (samples, time_steps, features)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Определение модели LSTM с Attention
def build_model(time_steps, features):
    inputs = Input(shape=(time_steps, features))
    
    # LSTM слой
    lstm_out, state_h, state_c = LSTM(50, return_sequences=True, return_state=True)(inputs)
    
    # Механизм внимания
    attention = dot([lstm_out, lstm_out], axes=[2, 2])  # Вычисление внимания
    attention = Activation('softmax')(attention)  # Нормализация через softmax
    context = dot([attention, lstm_out], axes=[2, 1])  # Применение внимания к LSTM выходам
    
    # Объединение контекста и состояний
    combined = concatenate([context, lstm_out])
    
    # Финальные Dense слои для прогноза
    output = TimeDistributed(Dense(1))(combined)
    
    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam', loss='mse')
    
    return model

# Параметры модели
model = build_model(time_steps, 1)

# Обучение модели
history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))

# Прогнозирование
y_pred = model.predict(X_test)

# Инвертирование масштабирования для получения исходных значений
y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1))
y_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))

# Визуализация результатов
plt.plot(y_test_inverse, label='Actual')
plt.plot(y_pred_inverse, label='Predicted')
plt.title('Temperature Prediction with Attention Mechanism')
plt.legend()
plt.show()